
![](https://img.shields.io/badge/-ChatGPT-412991?style=flat-square&logo=openai&logoColor=FFFFFF)

![](https://simpleicons.org/?modal=icon&q=awesomelists)

# backdoor-attack



This Github repository summarizes a list of **Backdoor Learning** resources.I will try my best to continuously maintain this Github Repository in a monthly manner.

#### Why is Backdoor Learning?

Backdoor learning is an emerging research area, which discusses the security issues of the training process towards machine learning algorithms. It is critical for safely adopting third-party training resources or models in reality.

Note: 'Backdoor' is also commonly called the 'Neural Trojan' or 'Trojan'.

---

## ATTACK

### TPAMI

#### 2024

* Robust and Transferable Backdoor Attacks Against Deep Image Compression With Selective Frequency Prior‚öîÔ∏è      [paper](https://ieeexplore.ieee.org/document/10771646)



---

### CVPR 

#### 2024

* BadCLIP: Dual-Embedding Guided Backdoor Attack on Multimodal Contrastive Learning‚öîÔ∏è    [paper](https://openaccess.thecvf.com/content/CVPR2024/html/Liang_BadCLIP_Dual-Embedding_Guided_Backdoor_Attack_on_Multimodal_Contrastive_Learning_CVPR_2024_paper.html) |  [code](https://github.com/LiangSiyuan21/BadCLIP)

* BadCLIP: Trigger-Aware Prompt Learning for Backdoor Attacks on CLIP‚öîÔ∏è   [paper](https://openaccess.thecvf.com/content/CVPR2024/html/Bai_BadCLIP_Trigger-Aware_Prompt_Learning_for_Backdoor_Attacks_on_CLIP_CVPR_2024_paper.html)    |     [code](https://github.com/jiawangbai/BadCLIP)



----

### ICLR

#### 2024

*  Universal Backdoor Attacks‚öîÔ∏è  [paper](https://openreview.net/forum?id=3QkzYBSWqL)|  [code](https://github.com/Ben-Schneider-code/Universal-Backdoor-Attacks)

---



### AAAL

#### 2024

* Progressive Poisoned Data Isolation for Training-Time Backdoor     [paper](https://ojs.aaai.org/index.php/AAAI/article/view/29023)    |    [code](https://github.com/RorschachChen/PIPD.git)

---

### ICML 

#### 2024

* Generalization Bound and New Algorithm for Clean-Label Backdoor Attack     [paper](https://openreview.net/forum?id=ZdqiT0McON)    | [code](https://github.com/hong-xian/backdoor-attack.git)
* A Theoretical Analysis of Backdoor Poisoning Attacks in Convolutional Neural Networks     [paper](https://openreview.net/forum?id=SfcB4cVvPz) 
* Causality Based Front-door Defense Against Backdoor Attack on Language Models üõ°Ô∏è        [paper](https://openreview.net/forum?id=dmHHVcHFdM) |     [code](https://github.com/lyr17/Frontdoor-Adjustment-Backdoor-Elimination)
* Energy-based Backdoor Defense without Task-Specific Samples and Model Retrainingüõ°Ô∏è      [paper](https://openreview.net/pdf?id=TJ6tVNt6Y4)    |  [code](https://github.com/ifen1/EBBA)

----

### NeurIPS

#### 2024

* Data Free Backdoor Attacks‚öîÔ∏è       [paper](https://papers.nips.cc/paper_files/paper/2024/file/2a7e91c6e4b68325d9884a7469804837-Paper-Conference.pdf)    |   [code](https://github.com/AAAAAAsuka/DataFree_Backdoor_Attacks)

* WaveAttack: Asymmetric Frequency Obfuscation-based Backdoor Attacks Against Deep Neural Networks‚öîÔ∏è    [paper](https://papers.nips.cc/paper_files/paper/2024/hash/4ce18228ececb78bca04cbce069891b1-Abstract-Conference.html)    |    [code](https://github.com/BililiCode/WaveAttack)

* BackTime: Backdoor Attacks on Multivariate Time Series Forecasting‚öîÔ∏è    [paper](https://papers.nips.cc/paper_files/paper/2024/hash/ed3cd2520148b577039adfade82a5566-Abstract-Conference.html)  |    [code](https://github.com/xiaolin-cs/BackTime)

* Causality Based Front-door Defense Against Backdoor Attack on Language Models    [paper](https://openreview.net/forum?id=dmHHVcHFdM)    |    [code](https://github.com/lyr17/Frontdoor-Adjustment-Backdoor-Elimination)

* Defense against Backdoor Attack on Pre-trained Language Models via Head Pruning and Attention Normalization    [paper](https://openreview.net/forum?id=1SiEfsCecd)   |    [code](https://github.com/xingyizhao/PURE)

* BD-PSC: Input-level Backdoor Detection via Parameter-oriented Scaling Consistency    [paper](https://openreview.net/forum?id=YCzbfs2few)   |   [code](https://github.com/THUYimingLi/BackdoorBox)

* Better Safe than Sorry: Pre-training CLIP against Targeted Data Poisoning and Backdoor Attacks    [paper](https://openreview.net/forum?id=ycLHJuLYuD)   |     [code](https://github.com/BigML-CS-UCLA/SafeCLIP)

  

* Energy-based Backdoor Defense without Task-Specific Samples and Model Retraining üõ°Ô∏è   [paper](https://openreview.net/forum?id=TJ6tVNt6Y4)   |  [code](https://github.com/ifen1/EBBA)

* Generalization Bound and New Algorithm for Clean-Label Backdoor Attack üõ°Ô∏è   [paper](https://openreview.net/forum?id=ZdqiT0McON)   | [paper](https://github.com/hong-xian/backdoor-attack)

* LT-Defense: Searching-free Backdoor Defense via Exploiting the Long-tailed Effect üõ°Ô∏è     [paper](https://papers.nips.cc/paper_files/paper/2024/file/064f6bcd7d3c72fb187bfca35ba2bfd4-Paper-Conference.pdf)    

* Uncovering, Explaining, and Mitigating the Superficial Safety of Backdoor Defense üõ°Ô∏è [paper ](https://papers.nips.cc/paper_files/paper/2024/hash/8e8399e5e7aed601c9f135f40be26564-Abstract-Conference.html)   |  [code](https://github.com/aisafety-hkust/stable_backdoor_purification)

* Breaking the False Sense of Security in Backdoor Defense through Re-Activation Attack üõ°Ô∏è   [paper](https://papers.nips.cc/paper_files/paper/2024/hash/d06537b4b38ccf008a54559d2c56fa23-Abstract-Conference.html)    |  [code](https://github.com/JulieCarlon/Backdoor-Reactivation-Attack)


* Parameter Disparities Dissection for Backdoor Defense in Heterogeneous Federated Learning üõ°Ô∏è  [paper](Parameter Disparities Dissection for Backdoor Defense in Heterogeneous Federated Learning)  |   [code](https://github.com/wenkehuang/FDCR)

* SampDetox: Black-box Backdoor Defense via Perturbation-based Sample Detoxificationüõ°Ô∏è   [paper](https://papers.nips.cc/paper_files/paper/2024/hash/dbb5180957513805ebeea787b8c66ac9-Abstract-Conference.html)  |   [code](https://github.com/easywood0204/SampDetox)

* 


----

### ACL

#### 2024

*  Acquiring Clean Language Models from Backdoor Poisoned Datasets üõ°Ô∏è  [paper](https://aclanthology.org/2024.acl-long.441.pdf)    |     [code](https://github.com/ZrW00/MuScleLoRA)

* BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents‚öîÔ∏è     [paper](https://aclanthology.org/2024.acl-long.530.pdf)    |     [code](https://github.com/DPamK/BadAgent)

* BadActs: A Universal Backdoor Defense in the Activation Spaceüõ°Ô∏è     [paper](https://aclanthology.org/2024.findings-acl.317.pdf)    | [code](https://github.com/clearloveclearlove/BadActs)

* UOR: Universal Backdoor Attacks on Pre-trained Language Models     [paper](https://aclanthology.org/2024.findings-acl.468/)     

---

### IJCAL

#### 2024

* BadFusion: 2D-Oriented Backdoor Attacks against 3D Object Detection‚öîÔ∏è     [paper](https://www.ijcai.org/proceedings/2024/39)  
* BADFSS: Backdoor Attacks on Federated Self-Supervised Learning ‚öîÔ∏è    [paper](https://www.ijcai.org/proceedings/2024/61)

# others

* 2024ACL-ARDEN: Multi-Directional Backdoor Watermarks for Embedding-as-a-Service Copyright Protection [paper](https://aclanthology.org/2024.acl-long.725.pdf)   |    [code](https://github.com/anudeex/WARDEN/blob/main/preparation/word_count.py)

* CausalDiff: Causality-Inspired Disentanglement via Diffusion Model for Adversarial Defense üõ°Ô∏è     [paper](https://papers.nips.cc/paper_files/paper/2024/hash/e1b619a9e241606a23eb21767f16cf81-Abstract-Conference.html)    |    [code](https://github.com/CAS-AISafetyBasicResearchGroup/CausalDiff)

* PureGen: Universal Data Purification for Train-Time Poison Defense via Generative Model Dynamics    [paper](https://papers.nips.cc/paper_files/paper/2024/hash/f4757db82a02eea015670ecca605d5cc-Abstract-Conference.html) |    [code](https://github.com/SunayBhat1/PureGen_PoisonDefense)

* PsySafe: A Comprehensive Framework for Psychological-based Attack, Defense, and Evaluation of Multi-agent System Safety    [paper](https://aclanthology.org/2024.acl-long.812/)   |    [code](https://github.com/AI4Good24/PsySafe)

* Data-free Defense of Black Box Models Against Adversarial Attacks     [paper](https://ieeexplore.ieee.org/document/10678045)     [paper1](https://arxiv.org/abs/2211.01579)  |     [code](https://github.com/vcl-iisc/data-free-black-box-defense)

* SignSGD with Federated Defense: Harnessing Adversarial Attacks through Gradient Sign Decoding     [paper](https://openreview.net/forum?id=zEqeNEuiJr)    | 

* Adversarially Robust Deep Multi-View Clustering: A Novel Attack and Defense Framework    [paper](https://openreview.net/forum?id=D9EfAkQCzh)    |    [code](https://github.com/libertyhhn/AR-DMVC)

* Defense against Model Extraction Attack by Bayesian Active Watermarking     [paper](https://openreview.net/pdf?id=EFtNP211X3)    

* Spear and Shield: Adversarial Attacks and Defense Methods for Model-Based Link Prediction on Continuous-Time Dynamic Graphs     [paper](https://doi.org/10.1609/aaai.v38i12.29239)     |     [code](https://github.com/wooner49/T-spear-shield)

* Value at Adversarial Risk: A Graph Defense Strategy against Cost-Aware Attacks     [paper](https://ojs.aaai.org/index.php/AAAI/article/view/29282)    |     [code]([(https://github.com/songwdfu/RisKeeper))

* Optimal Attack and Defense for Reinforcement Learning    [paper](https://ojs.aaai.org/index.php/AAAI/article/view/29282)   | [code](https://github.com/jermcmahan/Attack-Defense)

* Detection and Defense of Unlearnable Examples     [paper](https://ojs.aaai.org/index.php/AAAI/article/view/29667)     | [code](https://github.com/hala64/udp)

* Robust Communicative Multi-Agent Reinforcement Learning with Active Defense    [paper](https://ojs.aaai.org/index.php/AAAI/article/view/29708)   |    

* 

  (**The above papers are all from top conferences, such as CVPR, ACL, ICML, ICCV, etc.**)

---

* Unicode: attack-‚öîÔ∏è ;  defense-üõ°Ô∏è 
